\chapter{Planificación Temporal}\label{ch:temporal_scheduling}

\section{Cronología del Desarrollo}

La organización del desarrollo del proyecto siguió una estructura metodológica dividida en fases claramente diferenciadas. Se implementó una planificación temporal que permitió el desarrollo progresivo de todos los componentes del sistema, considerando las dependencias técnicas entre módulos y los requisitos normativos específicos. La metodología adoptada facilitó el control de la complejidad técnica y garantizó el cumplimiento de los objetivos establecidos dentro del cronograma propuesto.

\subsection{Fase de Investigación (Enero 2025)}

Durante enero se establecieron las bases teóricas y técnicas del proyecto mediante una fase de investigación exhaustiva. Se realizó el análisis de las tecnologías disponibles y se evaluaron los recursos necesarios para el desarrollo:

\begin{itemize}
    \item \textbf{Estudio de arquitecturas de redes neuronales}: Se realizó un análisis exhaustivo de diferentes arquitecturas de aprendizaje profundo, centrándose especialmente en las redes LSTM (Long Short-Term Memory) por su adecuación para el procesamiento de secuencias temporales como las señales EEG. Se evaluaron las características específicas que hacían estas arquitecturas apropiadas para el procesamiento de datos neurofisiológicos.
    
    \item \textbf{Evaluación de dispositivos EEG}: Se compararon diferentes dispositivos disponibles en el mercado, analizando criterios como precisión, número de canales, facilidad de integración y compatibilidad con bibliotecas de software existentes. Se seleccionó el dispositivo BrainBit por ofrecer un equilibrio óptimo entre funcionalidad, costo y disponibilidad de documentación técnica.
    
    \item \textbf{Investigación de bibliotecas de adquisición de datos}: Se evaluaron múltiples bibliotecas para la captura de datos EEG, seleccionando finalmente BrainFlow debido a su compatibilidad con diversos dispositivos y la robustez de su documentación técnica, factores críticos para el desarrollo de aplicaciones médicas.
    
    \item \textbf{Estudio de normativas aplicables}: Se analizaron las regulaciones y estándares relevantes para dispositivos médicos, prestando especial atención a la norma UNE-EN 62304:2007 para software de dispositivos médicos. Este análisis permitió establecer los requisitos de desarrollo y documentación necesarios.
    
    \item \textbf{Estudio de plataformas para implementación}: Se evaluaron diferentes opciones de hardware para la implementación del sistema, analizando capacidad de procesamiento en tiempo real y adecuación para aplicaciones médicas. Esta evaluación determinó los requisitos técnicos para la selección posterior del hardware.
\end{itemize}

\subsection{Adquisición de Hardware y Estructuración (Finales de Enero 2025)}

Completada la fase de investigación, se procedió a la adquisición del hardware necesario y la estructuración del proyecto. Esta fase se centró en materializar las decisiones técnicas tomadas durante la investigación y establecer la arquitectura base del sistema:

\begin{itemize}
    \item \textbf{Adquisición del dispositivo BrainBit}: Se obtuvo el dispositivo EEG seleccionado como fuente principal de datos neurofisiológicos para el proyecto. La adquisición se realizó considerando los criterios técnicos establecidos durante la fase de investigación.
    
    \item \textbf{Obtención de la Raspberry Pi 4}: Se seleccionó esta plataforma como sistema de implementación principal debido al equilibrio entre potencia de procesamiento, portabilidad y adecuación para el desarrollo de prototipos médicos.
    
    \item \textbf{Adquisición de bombillas inteligentes Tapo}: Se obtuvieron los dispositivos de control domótico necesarios para implementar las respuestas del sistema a las señales cerebrales procesadas. Esta selección se basó en la facilidad de integración y compatibilidad con los protocolos de comunicación requeridos.
    
    \item \textbf{Definición de la arquitectura del software}: Se diseñó la estructura general del proyecto adoptando una arquitectura hexagonal (puertos y adaptadores) para garantizar modularidad, testabilidad y facilidad de mantenimiento. Esta decisión arquitectónica se fundamentó en los requisitos normativos de la UNE-EN 62304.
    
    \item \textbf{Planificación de componentes del sistema}: Se definieron los módulos constituyentes del proyecto: neural\_analytics\_data para la captura de datos, neural\_analytics\_model para entrenamiento e inferencia, neural\_analytics\_core para la lógica central, y neural\_analytics\_gui para la interfaz de usuario. Esta modularización facilitó el desarrollo paralelo y la validación independiente de componentes.
\end{itemize}

\subsection{Fase de Desarrollo (Febrero - Marzo 2025)}

Durante febrero y marzo se implementaron todos los componentes del sistema siguiendo la arquitectura previamente definida. Esta fase constituyó el núcleo del desarrollo técnico, caracterizada por la implementación simultánea de múltiples módulos y su posterior integración:

\subsubsection{Desarrollo del Programa de Extracción de Datos (neural\_analytics\_data)}

Se desarrolló este módulo como primer componente del sistema, estableciendo la interfaz entre el dispositivo de adquisición y el resto del sistema. El desarrollo de este módulo presentó varios desafíos técnicos relacionados con la captura fiable de datos EEG:

\begin{itemize}
    \item \textbf{Integración con BrainFlow}: Se implementó la interfaz con la biblioteca BrainFlow para la captura de datos del dispositivo BrainBit. Se desarrolló un sistema de comunicación robusto que garantiza la adquisición continua y fiable de señales EEG.
    
    \item \textbf{Diseño de protocolos de captura}: Se desarrollaron rutinas estructuradas para la captura de datos EEG durante tareas de imaginación de colores. Se establecieron protocolos temporales precisos para garantizar la consistencia de las muestras y facilitar el posterior entrenamiento del modelo.
    
    \item \textbf{Implementación de procesamiento de señales}: Aquí tuve que sumergirme en un mundo completamente nuevo para mí. Desarrollé funciones para filtrar y preprocesar las señales raw, pero admito que al principio no tenía ni idea de lo que estaba haciendo. Me pasé días enteros leyendo sobre transformadas de Fourier y filtros digitales—cosas que sonaban muy impresionantes pero que en la práctica eran bastante intimidantes.
    
    \item \textbf{Almacenamiento y etiquetado de datos}: Implementé un sistema para almacenar y etiquetar automáticamente todo lo que iba capturando. Era súper importante que fuera automático porque sabía que si tenía que hacerlo manualmente, iba a meter la pata constantemente. Aunque me costó más trabajo del esperado hacer que el etiquetado fuera realmente confiable.
\end{itemize}

\subsubsection{Desarrollo del Programa de Entrenamiento del Modelo (neural\_analytics\_model)}

Esta parte me pilló desprevenido completamente. Mientras intentaba resolver los problemas de extracción de datos, se me ocurrió la brillante idea de desarrollar el módulo de entrenamiento en paralelo. En retrospectiva no sé ni cómo se me ocurrió hacer dos cosas tan complejas a la vez, pero sorprendentemente no fue tan caótico como sonaba en teoría. 

Lo que me funcionó fue ir probando el pipeline de entrenamiento con los primeros datos que conseguía extraer, aunque fuera información parcial. Esto me permitió detectar problemas mucho antes—como cuando me di cuenta de que el preprocesamiento que estaba haciendo eliminaba información crucial para la clasificación. Cosas que solo descubres cuando intentas entrenar con datos reales.

El módulo de entrenamiento se convirtió en una especie de laboratorio personal donde experimenté con muchas configuraciones diferentes:

\begin{itemize}
    \item \textbf{Diseño de arquitectura LSTM}: Aquí entré en territorio completamente desconocido para mí. Tuve que diseñar una red neuronal basada en capas LSTM optimizada específicamente para clasificar patrones en señales EEG, pero la verdad es que al principio no tenía ni idea de por dónde empezar. Estuve varias semanas leyendo papers y probando configuraciones casi aleatoriamente hasta que empecé a entender qué parámetros realmente importaban. Hubo días enteros en que pensaba haber encontrado la arquitectura perfecta, solo para descubrir al día siguiente que no funcionaba nada bien con datos diferentes.
    
    \item \textbf{Implementación en PyTorch}: Elegí PyTorch básicamente porque había leído que era más intuitivo que TensorFlow, pero enfrentarme a su sintaxis viniendo de otros frameworks fue como aprender a conducir después de montar en bicicleta toda la vida. Las primeras semanas fueron brutales—cada cosa sencilla que quería hacer me llevaba horas porque no entendía bien cómo funcionaban los tensores. Hubo noches en que me quedé despierto solo intentando que un simple entrenamiento funcionara sin errores.
    
    \item \textbf{Rutinas de entrenamiento y validación}: Esta fue la parte más artesanal de todo el proceso. Desarrollé procedimientos para entrenar el modelo de manera eficiente y validarlo con diferentes conjuntos de datos, pero conseguir que el entrenamiento fuera estable se convirtió en mi pesadilla personal. El modelo tenía días buenos y días malos—a veces convergía perfectamente en pocas épocas, y otras veces se negaba a aprender nada coherente por más que ajustara parámetros. Era desesperante.
    
    \item \textbf{Exportación a formato ONNX}: Esto me costó más trabajo del esperado. Implementé la conversión para poder usar el modelo después en Rust, pero la compatibilidad entre PyTorch y ONNX resultó ser más problemática de lo que las documentaciones sugerían. Había operaciones que funcionaban perfectamente en PyTorch pero que ONNX no sabía cómo interpretar. Me pasé días enteros reescribiendo partes del modelo solo para que la exportación funcionara correctamente.
\end{itemize}

\subsubsection{Desarrollo del Core del Sistema (neural\_analytics\_core)}

Y aquí llegamos a lo que se convirtió en el boss final de todo el proyecto. Esta parte me consumió una cantidad absurda de tiempo—mucho más de lo que había calculado, y eso que ya había sido bastante conservador con mis estimaciones temporales. 

Implementar correctamente la arquitectura hexagonal se transformó en mi obsesión durante semanas enteras. Literalmente había días en que me despertaba pensando en puertos y adaptadores, y me iba a dormir con diagramas de arquitectura dándome vueltas en la cabeza como si fuera música pegadiza que no puedes sacar.

Se estableció la arquitectura núcleo del sistema implementando los principios de diseño hexagonal para garantizar la separación entre lógica de negocio e infraestructura. Se desarrolló un sistema modular que permite la interoperabilidad eficiente entre componentes:

\begin{itemize}
    \item \textbf{Implementación de puertos y adaptadores}: Se definieron interfaces claras para todos los componentes externos (puertos) y sus implementaciones concretas (adaptadores), siguiendo estrictamente los principios de la arquitectura hexagonal. Esta implementación garantiza la flexibilidad del sistema y facilita futuras extensiones o modificaciones.
    
    \item \textbf{Desarrollo del dominio central}: Se implementó la lógica de negocio central completamente independiente de las infraestructuras externas. Esta separación permite el desarrollo, testing y mantenimiento de cada componente de forma aislada, cumpliendo con los requisitos normativos de trazabilidad.
    
    \item \textbf{Sistema de eventos}: Se desarrolló un mecanismo de comunicación basado en eventos utilizando la biblioteca presage para Rust. Este sistema permite el desacoplamiento efectivo entre componentes y facilita la gestión del flujo de información a través del sistema.
    
    \item \textbf{Máquina de estados}: Se implementó una máquina de estados para gestionar el ciclo de vida de la aplicación y las transiciones entre diferentes modos operativos. Esta implementación garantiza el comportamiento predecible del sistema y facilita la gestión de estados de error y recuperación.
    
    \item \textbf{Servicio de inferencia}: Se desarrolló un servicio para la ejecución del modelo en tiempo real utilizando tract-onnx para inferencia en Rust. Esta implementación garantiza el rendimiento requerido para el procesamiento en tiempo real de señales EEG.
    
    \item \textbf{Control de dispositivos domóticos}: Se implementó la integración con dispositivos inteligentes a través de la biblioteca tapo. Esta integración permite el control remoto efectivo de dispositivos de iluminación basado en las predicciones del modelo.
\end{itemize}

\subsubsection{Desarrollo de la Interfaz Gráfica (neural\_analytics\_gui)}

Se desarrolló la interfaz gráfica del sistema utilizando tecnologías modernas para crear una experiencia de usuario eficiente y accesible. El desarrollo se centró en proporcionar visualización en tiempo real de las señales EEG y control intuitivo del sistema:

\begin{itemize}
    \item \textbf{Diseño de interfaz con Slint}: Se utilizó el framework Slint para crear una interfaz moderna y eficiente. Esta elección tecnológica permitió el desarrollo de una interfaz responsiva con renderizado de alto rendimiento, adecuada para el procesamiento en tiempo real.
    
    \item \textbf{Visualización de señales EEG}: Se implementó la representación gráfica de las señales en tiempo real utilizando la biblioteca plotters. Esta funcionalidad permite el monitoreo continuo de la calidad de las señales y facilita la calibración del sistema.
    
    \item \textbf{Integración con el core}: Se estableció la comunicación bidireccional con el núcleo del sistema para la visualización de estados y resultados en tiempo real. Esta integración garantiza la sincronización efectiva entre el frontend y el backend del sistema.
    
    \item \textbf{Interfaz para calibración}: Se desarrollaron vistas específicas para la calibración del dispositivo y verificación de impedancias. Estas funcionalidades son críticas para garantizar la calidad de las señales EEG y la fiabilidad de las predicciones.
    
    \item \textbf{Visualización de predicciones}: Se implementó un sistema para mostrar las predicciones del modelo en tiempo real de manera clara y comprensible. La interfaz incluye indicadores de confianza y visualización de estados del sistema para facilitar la interpretación de resultados.
\end{itemize}

\subsection{Fase de Refinamiento del Modelo (Abril 2025)}

Durante abril se implementó una fase intensiva de refinamiento del modelo de aprendizaje profundo. Esta fase se caracterizó por la optimización sistemática de todos los parámetros del modelo y la ampliación significativa del dataset de entrenamiento:

\begin{itemize}
    \item \textbf{Ampliación del dataset}: Se organizaron sesiones adicionales de captura de datos EEG para aumentar significativamente tanto el tamaño como la diversidad del dataset disponible. Se incluyeron grabaciones de múltiples usuarios en diferentes condiciones temporales para mejorar la capacidad de generalización del modelo.
    
    \item \textbf{Diversificación de casos de uso}: Se expandieron los escenarios de prueba para incluir variaciones en los enfoques mentales utilizados por diferentes usuarios durante las tareas de imaginación de colores. Esta diversificación permitió al modelo adaptarse a diferentes estrategias cognitivas individuales.
    
    \item \textbf{Reentrenamiento del modelo}: Se realizaron múltiples iteraciones de reentrenamiento del modelo LSTM utilizando los datos ampliados. Se optimizaron sistemáticamente los hiperparámetros para maximizar la precisión del modelo manteniendo el rendimiento en tiempo real.
    
    \item \textbf{Validación cruzada}: Se implementaron técnicas rigurosas de validación cruzada para verificar la consistencia del rendimiento del modelo con diferentes subconjuntos de datos. Esta metodología garantiza la fiabilidad de las métricas de rendimiento obtenidas.
    
    \item \textbf{Ajuste de umbrales de confianza}: Se refinaron los mecanismos para determinar cuándo una predicción debe clasificarse como "desconocida" en lugar de forzar una clasificación incorrecta. Esta optimización mejoró significativamente la fiabilidad percibida del sistema durante operación en condiciones de incertidumbre.
\end{itemize}

\section{Distribución Temporal}

La distribución temporal del proyecto refleja la complejidad técnica de cada fase y las dependencias entre componentes. Los tiempos de desarrollo se organizaron de manera eficiente para optimizar el uso de recursos:

\begin{table}[ht]
    \centering
    \begin{tabular}{|l|c|c|}
        \hline
        \textbf{Fase} & \textbf{Período} & \textbf{Duración} \\
        \hline
        Investigación & Enero 2025 & 4 semanas \\
        \hline
        Adquisición y Estructuración & Finales de Enero 2025 & 1 semana \\
        \hline
        Desarrollo del Programa de Extracción & Febrero 2025 & 2 semanas \\
        \hline
        Desarrollo del Programa de Entrenamiento & Febrero 2025 & 2 semanas \\
        \hline
        Desarrollo del Core del Sistema & Febrero - Marzo 2025 & 4 semanas \\
        \hline
        Desarrollo de la Interfaz Gráfica & Marzo 2025 & 2 semanas \\
        \hline
        Pruebas Iniciales & Finales de Marzo 2025 & 2 semanas \\
        \hline
        Refinamiento del Modelo & Abril 2025 & 4 semanas \\
        \hline
    \end{tabular}
    \caption{Distribución temporal del desarrollo del proyecto Neural Analytics}
    \label{tab:temporal_distribution}
\end{table}

\newpage
\section{Diagrama de Gantt}

Aquí está la representación visual de cómo se fueron solapando las diferentes fases. Me gusta este tipo de diagramas porque realmente te permiten ver cómo se superpusieron las tareas en el tiempo—algo que no es tan obvio cuando solo miras las fechas:

\begin{figure}[ht]
    \centering
    \definecolor{barblue}{RGB}{153,204,254}
    \definecolor{groupblue}{RGB}{51,102,254}
    \definecolor{linkred}{RGB}{165,0,33}
    \definecolor{investigacion}{RGB}{51,102,204}
    \definecolor{adquisicion}{RGB}{60,179,113}
    \definecolor{extraccion}{RGB}{255,153,0}
    \definecolor{entrenamiento}{RGB}{255,128,0}
    \definecolor{core}{RGB}{204,0,0}
    \definecolor{interfaz}{RGB}{255,153,51}
    \definecolor{pruebas}{RGB}{153,51,153}
    \definecolor{refinamiento}{RGB}{0,204,204}
    
    \renewcommand\sfdefault{phv}
    \renewcommand\mddefault{mc}
    \renewcommand\bfdefault{bc}
    \sffamily
    \begin{ganttchart}[
        canvas/.append style={fill=none, draw=black!5, line width=.75pt},
        hgrid style/.style={draw=black!5, line width=.75pt},
        vgrid={*1{draw=black!5, line width=.75pt}},
        title/.style={draw=none, fill=none},
        title label font=\bfseries\footnotesize,
        title label node/.append style={below=4pt},
        include title in canvas=false,
        bar label font=\mdseries\small\color{black!70},
        bar label node/.append style={left=2cm},
        bar/.style={draw=none, rounded corners=1pt},
        bar height=0.7,
        y unit title=0.8cm,
        y unit chart=0.7cm,
        x unit=0.6cm,
        group left shift=0,
        group right shift=0,
        group height=.5,
        group peaks tip position=0
    ]{1}{17}
        \gantttitle[
          title label node/.append style={below left=7pt and -3pt}
        ]{\textbf{Planificación Neural Analytics 2025}}{17} \\
        \gantttitle{Enero}{4} 
        \gantttitle{Febrero}{4} 
        \gantttitle{Marzo}{5} 
        \gantttitle{Abril}{4} \\
        
        \ganttgroup[group/.style={fill=investigacion}]{Fase de Investigación}{1}{4} \\
        \ganttbar[name=invest, bar/.style={fill=investigacion!90}]{\textbf{Investigación}}{1}{4} \\[grid]
        
        \ganttgroup[group/.style={fill=adquisicion}]{Fase de Adquisición}{4}{5} \\
        \ganttbar[name=adqui, bar/.style={fill=adquisicion!90}]{\textbf{Adquisición y Estructuración}}{4}{5} \\[grid]
        
        \ganttgroup[group/.style={fill=extraccion}]{Fase de Desarrollo}{5}{15} \\
        \ganttbar[name=extract, bar/.style={fill=extraccion!90}]{\textbf{Extracción de Datos}}{5}{8} \\
        \ganttbar[name=entren, bar/.style={fill=entrenamiento!90}]{\textbf{Entrenamiento del Modelo}}{5}{8} \\
        \ganttbar[name=core, bar/.style={fill=core!90}]{\textbf{Core del Sistema}}{8}{13} \\
        \ganttbar[name=gui, bar/.style={fill=interfaz!90}]{\textbf{Interfaz Gráfica}}{13}{15} \\
        \ganttbar[name=test, bar/.style={fill=pruebas!90}]{\textbf{Pruebas Iniciales}}{13}{15} \\[grid]
        
        \ganttgroup[group/.style={fill=refinamiento}]{Fase de Refinamiento}{14}{17} \\
        \ganttbar[name=refine, bar/.style={fill=refinamiento!90}]{\textbf{Refinamiento del Modelo}}{14}{17} \\
        
        \ganttlink[link/.style={-latex, line width=1pt, black!40}]{adqui}{extract}
        \ganttlink[link/.style={-latex, line width=1pt, black!40}]{extract}{core}
        \ganttlink[link/.style={-latex, line width=1pt, black!40}]{core}{gui}
        \ganttlink[link/.style={-latex, line width=1pt, black!40}]{gui}{refine}
    \end{ganttchart}
    \caption{Diagrama de Gantt del proyecto Neural Analytics}
    \label{fig:gantt_diagram}
\end{figure}

\section{Conclusiones sobre la Planificación}

La evaluación retrospectiva de la planificación temporal del proyecto permite identificar aspectos exitosos y áreas de mejora para futuros desarrollos similares. La metodología implementada resultó adecuada para alcanzar los objetivos establecidos dentro del marco temporal propuesto.

\begin{itemize}
    \item \textbf{Duración de la fase de desarrollo core}: El desarrollo del núcleo del sistema siguiendo estrictamente los principios de arquitectura hexagonal requirió una inversión temporal considerable. Esta inversión se justificó por los beneficios obtenidos en términos de mantenibilidad, testabilidad y facilidad para realizar pruebas exhaustivas posteriores.
    
    \item \textbf{Paralelización de tareas}: La estructuración del desarrollo en componentes modulares desde el inicio permitió el desarrollo simultáneo de varios módulos. Esta estrategia optimizó significativamente el tiempo total de desarrollo y facilitó la gestión de la complejidad del proyecto.
    
    \item \textbf{Importancia de la fase de refinamiento}: La fase de abril demostró ser más crítica de lo inicialmente estimado. La ampliación del dataset con muestras diversas y representativas incrementó significativamente el rendimiento del modelo, justificando la inversión temporal adicional requerida.
    
    \item \textbf{Iteración continua}: El enfoque iterativo resultó fundamental, especialmente durante la etapa de refinamiento del modelo. Cada incorporación de datos permitió ajustes incrementales que mejoraron gradualmente el rendimiento del sistema.
    
    \item \textbf{Áreas de mejora identificadas}: Para futuros desarrollos similares, se recomienda ampliar significativamente la fase de pruebas con usuarios reales para obtener mayor retroalimentación sobre la usabilidad del sistema. También se sugiere continuar expandiendo el dataset con muestras más diversas y representativas.
\end{itemize}

La implementación de la arquitectura hexagonal, aunque inicialmente más costosa en tiempo de desarrollo, proporcionó una base sólida para cumplir efectivamente con los requisitos normativos y facilitar futuras extensiones del sistema.

La dedicación de un mes completo al refinamiento intensivo del modelo fue fundamental para alcanzar niveles de precisión adecuados para un dispositivo de uso médico. La experiencia obtenida sugiere que esta fase crítica debería considerarse con mayor peso temporal en planificaciones futuras de proyectos similares.